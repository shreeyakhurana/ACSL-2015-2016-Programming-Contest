{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQAstXRA5Elp",
        "outputId": "78a364d8-2315-4ade-ee82-9657c9e9687e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'small-coder'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 40 (delta 8), reused 17 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (40/40), 46.73 KiB | 3.11 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "/content/small-coder\n"
          ]
        }
      ],
      "source": [
        "![ ! -f requirements.txt ] && git clone https://github.com/anudaweerasinghe/small-coder.git\n",
        "%cd /content/small-coder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gEAAaukh5Elr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "! pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bigcode-project/bigcode-evaluation-harness.git\n",
        "%cd bigcode-evaluation-harness"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biOzMPt4y5IY",
        "outputId": "0bfda60b-3471-4b08-e640-443933a34934"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bigcode-evaluation-harness'...\n",
            "remote: Enumerating objects: 4107, done.\u001b[K\n",
            "remote: Counting objects: 100% (1689/1689), done.\u001b[K\n",
            "remote: Compressing objects: 100% (487/487), done.\u001b[K\n",
            "remote: Total 4107 (delta 1360), reused 1381 (delta 1193), pack-reused 2418\u001b[K\n",
            "Receiving objects: 100% (4107/4107), 818.20 KiB | 9.51 MiB/s, done.\n",
            "Resolving deltas: 100% (2794/2794), done.\n",
            "/content/small-coder/bigcode-evaluation-harness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "NrdHKbyszgyy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config default\n",
        "!mv /root/.cache/huggingface/accelerate/default_config.yaml /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgZuU9OGzmhb",
        "outputId": "617b6fd4-5922-4d34-cfe9-287dbf876f38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch  main.py \\\n",
        "  --model \"deepseek-ai/deepseek-coder-1.3b-instruct\" \\\n",
        "  --max_length_generation 1024 \\\n",
        "  --tasks humaneval \\\n",
        "  --temperature 0.2 \\\n",
        "  --n_samples 1 \\\n",
        "  --batch_size 1 \\\n",
        "  --allow_code_execution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRRgCeYp0rrR",
        "outputId": "e92a2e5d-6978-4a4f-e111-73391a9875b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-04-13 00:21:45.402544: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-13 00:21:45.402590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-13 00:21:45.403895: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-13 00:21:46.523923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Selected Tasks: ['humaneval']\n",
            "Loading model in fp32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "number of problems for this task is 164\n",
            "100% 164/164 [10:57<00:00,  4.01s/it]\n",
            "Evaluating generations...\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "{\n",
            "  \"humaneval\": {\n",
            "    \"pass@1\": 0.5975609756097561\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"prefix\": \"\",\n",
            "    \"do_sample\": true,\n",
            "    \"temperature\": 0.2,\n",
            "    \"top_k\": 0,\n",
            "    \"top_p\": 0.95,\n",
            "    \"n_samples\": 1,\n",
            "    \"eos\": \"<|endoftext|>\",\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
            "    \"modeltype\": \"causal\",\n",
            "    \"peft_model\": null,\n",
            "    \"revision\": null,\n",
            "    \"use_auth_token\": false,\n",
            "    \"trust_remote_code\": false,\n",
            "    \"tasks\": \"humaneval\",\n",
            "    \"instruction_tokens\": null,\n",
            "    \"batch_size\": 1,\n",
            "    \"max_length_generation\": 1024,\n",
            "    \"precision\": \"fp32\",\n",
            "    \"load_in_8bit\": false,\n",
            "    \"load_in_4bit\": false,\n",
            "    \"left_padding\": false,\n",
            "    \"limit\": null,\n",
            "    \"limit_start\": 0,\n",
            "    \"save_every_k_tasks\": -1,\n",
            "    \"postprocess\": true,\n",
            "    \"allow_code_execution\": true,\n",
            "    \"generation_only\": false,\n",
            "    \"load_generations_path\": null,\n",
            "    \"load_data_path\": null,\n",
            "    \"metric_output_path\": \"evaluation_results.json\",\n",
            "    \"save_generations\": false,\n",
            "    \"load_generations_intermediate_paths\": null,\n",
            "    \"save_generations_path\": \"generations.json\",\n",
            "    \"save_references\": false,\n",
            "    \"save_references_path\": \"references.json\",\n",
            "    \"prompt\": \"prompt\",\n",
            "    \"max_memory_per_gpu\": null,\n",
            "    \"check_references\": false\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch  main.py \\\n",
        "  --model \"deepseek-ai/deepseek-coder-1.3b-instruct\" \\\n",
        "  --max_length_generation 256000 \\\n",
        "  --tasks mbpp \\\n",
        "  --temperature 0.2 \\\n",
        "  --n_samples 1 \\\n",
        "  --batch_size 1 \\\n",
        "  --allow_code_execution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq6rDO_B4Lrg",
        "outputId": "51c9f7d7-6840-4447-ac31-5cd1b191806b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-04-15 02:35:31.871085: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-15 02:35:31.871133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-15 02:35:31.872600: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-15 02:35:32.885254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Selected Tasks: ['mbpp']\n",
            "Loading model in fp32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "config.json: 100% 631/631 [00:00<00:00, 3.42MB/s]\n",
            "model.safetensors: 100% 2.69G/2.69G [00:13<00:00, 195MB/s]\n",
            "generation_config.json: 100% 119/119 [00:00<00:00, 686kB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 1.87k/1.87k [00:00<00:00, 10.3MB/s]\n",
            "tokenizer.json: 100% 1.37M/1.37M [00:00<00:00, 2.68MB/s]\n",
            "Downloading readme: 100% 9.06k/9.06k [00:00<00:00, 18.5MB/s]\n",
            "Downloading data: 100% 87.2k/87.2k [00:00<00:00, 308kB/s]\n",
            "Downloading data: 100% 116k/116k [00:00<00:00, 797kB/s]\n",
            "Downloading data: 100% 25.1k/25.1k [00:00<00:00, 173kB/s]\n",
            "Downloading data: 100% 7.88k/7.88k [00:00<00:00, 59.6kB/s]\n",
            "Generating train split: 100% 374/374 [00:00<00:00, 5279.51 examples/s]\n",
            "Generating test split: 100% 500/500 [00:00<00:00, 70511.47 examples/s]\n",
            "Generating validation split: 100% 90/90 [00:00<00:00, 22248.33 examples/s]\n",
            "Generating prompt split: 100% 10/10 [00:00<00:00, 3989.26 examples/s]\n",
            "number of problems for this task is 500\n",
            "100% 500/500 [19:16<00:00,  2.31s/it]\n",
            "Evaluating generations...\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "{\n",
            "  \"mbpp\": {\n",
            "    \"pass@1\": 0.448\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"prefix\": \"\",\n",
            "    \"do_sample\": true,\n",
            "    \"temperature\": 0.2,\n",
            "    \"top_k\": 0,\n",
            "    \"top_p\": 0.95,\n",
            "    \"n_samples\": 1,\n",
            "    \"eos\": \"<|endoftext|>\",\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
            "    \"modeltype\": \"causal\",\n",
            "    \"peft_model\": null,\n",
            "    \"revision\": null,\n",
            "    \"use_auth_token\": false,\n",
            "    \"trust_remote_code\": false,\n",
            "    \"tasks\": \"mbpp\",\n",
            "    \"instruction_tokens\": null,\n",
            "    \"batch_size\": 1,\n",
            "    \"max_length_generation\": 256000,\n",
            "    \"precision\": \"fp32\",\n",
            "    \"load_in_8bit\": false,\n",
            "    \"load_in_4bit\": false,\n",
            "    \"left_padding\": false,\n",
            "    \"limit\": null,\n",
            "    \"limit_start\": 0,\n",
            "    \"save_every_k_tasks\": -1,\n",
            "    \"postprocess\": true,\n",
            "    \"allow_code_execution\": true,\n",
            "    \"generation_only\": false,\n",
            "    \"load_generations_path\": null,\n",
            "    \"load_data_path\": null,\n",
            "    \"metric_output_path\": \"evaluation_results.json\",\n",
            "    \"save_generations\": false,\n",
            "    \"load_generations_intermediate_paths\": null,\n",
            "    \"save_generations_path\": \"generations.json\",\n",
            "    \"save_references\": false,\n",
            "    \"save_references_path\": \"references.json\",\n",
            "    \"prompt\": \"prompt\",\n",
            "    \"max_memory_per_gpu\": null,\n",
            "    \"check_references\": false\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-2 base model results: HumanEval\n",
        "!accelerate launch  main.py \\\n",
        "  --model \"microsoft/phi-2\" \\\n",
        "  --max_length_generation 1024 \\\n",
        "  --tasks humaneval \\\n",
        "  --temperature 0.2 \\\n",
        "  --n_samples 1 \\\n",
        "  --batch_size 1 \\\n",
        "  --allow_code_execution"
      ],
      "metadata": {
        "id": "WseLbEH04VhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b14bdd-6789-4929-f2e0-748aff253253"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-04-15 03:01:02.780727: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-15 03:01:02.780785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-15 03:01:02.782915: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-15 03:01:04.410232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Selected Tasks: ['humaneval']\n",
            "Loading model in fp32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "config.json: 100% 863/863 [00:00<00:00, 4.55MB/s]\n",
            "model.safetensors.index.json: 100% 35.7k/35.7k [00:00<00:00, 439kB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/5.00G [00:00<00:17, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/5.00G [00:00<00:12, 381MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/5.00G [00:00<00:12, 382MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/5.00G [00:00<00:14, 343MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/5.00G [00:00<00:15, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/5.00G [00:00<00:15, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 294M/5.00G [00:00<00:13, 340MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/5.00G [00:00<00:12, 359MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/5.00G [00:01<00:19, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 409M/5.00G [00:01<00:22, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/5.00G [00:01<00:21, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 472M/5.00G [00:04<01:48, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/5.00G [00:04<01:23, 54.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 524M/5.00G [00:04<01:12, 61.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/5.00G [00:04<01:03, 70.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/5.00G [00:04<00:48, 91.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/5.00G [00:04<00:42, 104MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/5.00G [00:04<00:33, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/5.00G [00:04<00:31, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/5.00G [00:05<00:26, 164MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/5.00G [00:08<03:10, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 734M/5.00G [00:09<02:29, 28.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 755M/5.00G [00:09<01:57, 36.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/5.00G [00:09<01:35, 44.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/5.00G [00:09<01:14, 56.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/5.00G [00:09<00:53, 78.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 860M/5.00G [00:09<00:39, 104MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 891M/5.00G [00:09<00:33, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 912M/5.00G [00:10<00:32, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 944M/5.00G [00:10<00:25, 157MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 975M/5.00G [00:13<02:57, 22.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/5.00G [00:14<02:19, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.02G/5.00G [00:14<01:53, 34.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/5.00G [00:14<01:29, 44.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.07G/5.00G [00:14<01:02, 62.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/5.00G [00:14<00:51, 76.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/5.00G [00:14<00:37, 104MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/5.00G [00:14<00:29, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/5.00G [00:15<00:24, 156MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/5.00G [00:15<00:20, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/5.00G [00:15<00:18, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/5.00G [00:15<00:15, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/5.00G [00:15<00:14, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/5.00G [00:15<00:13, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/5.00G [00:15<00:12, 288MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/5.00G [00:15<00:12, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/5.00G [00:16<00:12, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.52G/5.00G [00:16<00:12, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/5.00G [00:16<00:13, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/5.00G [00:16<00:13, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/5.00G [00:16<00:13, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/5.00G [00:16<00:13, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/5.00G [00:16<00:12, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/5.00G [00:16<00:11, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/5.00G [00:17<00:12, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.77G/5.00G [00:17<00:12, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/5.00G [00:17<00:12, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/5.00G [00:17<00:12, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.87G/5.00G [00:17<00:12, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/5.00G [00:17<00:12, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/5.00G [00:17<00:12, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.97G/5.00G [00:18<00:15, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/5.00G [00:18<00:16, 182MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/5.00G [00:18<00:13, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/5.00G [00:18<00:13, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/5.00G [00:18<00:17, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/5.00G [00:18<00:16, 172MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.16G/5.00G [00:19<00:15, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/5.00G [00:19<00:13, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/5.00G [00:19<00:11, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.26G/5.00G [00:19<00:12, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/5.00G [00:19<00:13, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/5.00G [00:19<00:12, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/5.00G [00:20<00:14, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/5.00G [00:20<00:14, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/5.00G [00:21<00:37, 68.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.42G/5.00G [00:23<01:57, 21.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/5.00G [00:24<01:29, 28.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.46G/5.00G [00:24<01:09, 36.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/5.00G [00:24<00:53, 46.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/5.00G [00:24<00:41, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/5.00G [00:24<00:29, 84.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.57G/5.00G [00:24<00:22, 109MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/5.00G [00:24<00:20, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.61G/5.00G [00:24<00:17, 133MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/5.00G [00:25<00:14, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/5.00G [00:29<01:47, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/5.00G [00:29<01:28, 26.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/5.00G [00:29<00:53, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/5.00G [00:29<00:46, 48.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/5.00G [00:29<00:29, 74.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/5.00G [00:29<00:22, 95.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.86G/5.00G [00:29<00:18, 118MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/5.00G [00:30<00:16, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/5.00G [00:30<00:13, 150MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/5.00G [00:30<00:11, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/5.00G [00:30<00:11, 181MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.02G/5.00G [00:30<00:10, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/5.00G [00:30<00:09, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/5.00G [00:30<00:08, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.11G/5.00G [00:31<00:07, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/5.00G [00:31<00:07, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/5.00G [00:31<00:07, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.21G/5.00G [00:31<00:09, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/5.00G [00:32<00:29, 60.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.26G/5.00G [00:34<00:42, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/5.00G [00:34<00:30, 55.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/5.00G [00:34<00:22, 74.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/5.00G [00:34<00:19, 84.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.37G/5.00G [00:34<00:17, 91.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/5.00G [00:34<00:13, 116MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/5.00G [00:34<00:11, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/5.00G [00:34<00:10, 149MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.47G/5.00G [00:35<00:10, 152MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.50G/5.00G [00:35<00:08, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/5.00G [00:38<01:04, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/5.00G [00:39<00:57, 25.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.57G/5.00G [00:39<00:43, 33.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/5.00G [00:39<00:33, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.62G/5.00G [00:39<00:22, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/5.00G [00:39<00:16, 82.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/5.00G [00:39<00:12, 104MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.70G/5.00G [00:39<00:11, 110MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/5.00G [00:40<00:09, 136MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.76G/5.00G [00:40<00:07, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/5.00G [00:40<00:06, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/5.00G [00:40<00:05, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.86G/5.00G [00:40<00:04, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/5.00G [00:40<00:04, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/5.00G [00:40<00:04, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.95G/5.00G [00:40<00:03, 263MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/5.00G [00:40<00:03, 270MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.02G/5.00G [00:41<00:03, 268MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/5.00G [00:41<00:03, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/5.00G [00:41<00:03, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.11G/5.00G [00:41<00:03, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/5.00G [00:41<00:03, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/5.00G [00:41<00:02, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.20G/5.00G [00:41<00:02, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/5.00G [00:41<00:02, 286MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.27G/5.00G [00:41<00:02, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.30G/5.00G [00:42<00:03, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/5.00G [00:42<00:03, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.36G/5.00G [00:43<00:10, 63.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/5.00G [00:43<00:07, 80.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/5.00G [00:43<00:05, 99.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/5.00G [00:44<00:05, 106MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/5.00G [00:44<00:03, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.51G/5.00G [00:44<00:03, 157MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/5.00G [00:44<00:02, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/5.00G [00:45<00:07, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/5.00G [00:45<00:05, 69.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.61G/5.00G [00:45<00:04, 83.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/5.00G [00:46<00:03, 98.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.66G/5.00G [00:46<00:02, 114MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/5.00G [00:46<00:02, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.71G/5.00G [00:46<00:01, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/5.00G [00:46<00:01, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/5.00G [00:46<00:01, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.80G/5.00G [00:46<00:00, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/5.00G [00:46<00:00, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.87G/5.00G [00:46<00:00, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/5.00G [00:47<00:00, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.93G/5.00G [00:47<00:00, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.96G/5.00G [00:47<00:00, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 5.00G/5.00G [00:47<00:00, 105MB/s]\n",
            "Downloading shards:  50% 1/2 [00:47<00:47, 47.67s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/564M [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 41.9M/564M [00:00<00:01, 360MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 83.9M/564M [00:00<00:01, 293MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 126M/564M [00:00<00:01, 325MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 168M/564M [00:00<00:01, 342MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 210M/564M [00:00<00:01, 327MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 252M/564M [00:00<00:01, 283MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 283M/564M [00:01<00:01, 231MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 315M/564M [00:01<00:01, 165MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 346M/564M [00:01<00:01, 166MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 367M/564M [00:01<00:01, 150MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 398M/564M [00:01<00:00, 167MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 430M/564M [00:01<00:00, 193MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 461M/564M [00:02<00:00, 200MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 493M/564M [00:02<00:00, 198MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 524M/564M [00:02<00:00, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 564M/564M [00:06<00:00, 85.0MB/s]\n",
            "Downloading shards: 100% 2/2 [00:54<00:00, 27.25s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:22<00:00, 11.39s/it]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 627kB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 7.34k/7.34k [00:00<00:00, 28.9MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 50.8MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.85MB/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 5.04MB/s]\n",
            "added_tokens.json: 100% 1.08k/1.08k [00:00<00:00, 5.61MB/s]\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 506kB/s]\n",
            "Downloading readme: 100% 6.52k/6.52k [00:00<00:00, 16.9MB/s]\n",
            "Downloading data: 100% 83.9k/83.9k [00:00<00:00, 144kB/s]\n",
            "Generating test split: 100% 164/164 [00:00<00:00, 958.38 examples/s] \n",
            "number of problems for this task is 164\n",
            "100% 164/164 [10:13<00:00,  3.74s/it]\n",
            "Evaluating generations...\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "{\n",
            "  \"humaneval\": {\n",
            "    \"pass@1\": 0.4573170731707317\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"prefix\": \"\",\n",
            "    \"do_sample\": true,\n",
            "    \"temperature\": 0.2,\n",
            "    \"top_k\": 0,\n",
            "    \"top_p\": 0.95,\n",
            "    \"n_samples\": 1,\n",
            "    \"eos\": \"<|endoftext|>\",\n",
            "    \"seed\": 0,\n",
            "    \"model\": \"microsoft/phi-2\",\n",
            "    \"modeltype\": \"causal\",\n",
            "    \"peft_model\": null,\n",
            "    \"revision\": null,\n",
            "    \"use_auth_token\": false,\n",
            "    \"trust_remote_code\": false,\n",
            "    \"tasks\": \"humaneval\",\n",
            "    \"instruction_tokens\": null,\n",
            "    \"batch_size\": 1,\n",
            "    \"max_length_generation\": 1024,\n",
            "    \"precision\": \"fp32\",\n",
            "    \"load_in_8bit\": false,\n",
            "    \"load_in_4bit\": false,\n",
            "    \"left_padding\": false,\n",
            "    \"limit\": null,\n",
            "    \"limit_start\": 0,\n",
            "    \"save_every_k_tasks\": -1,\n",
            "    \"postprocess\": true,\n",
            "    \"allow_code_execution\": true,\n",
            "    \"generation_only\": false,\n",
            "    \"load_generations_path\": null,\n",
            "    \"load_data_path\": null,\n",
            "    \"metric_output_path\": \"evaluation_results.json\",\n",
            "    \"save_generations\": false,\n",
            "    \"load_generations_intermediate_paths\": null,\n",
            "    \"save_generations_path\": \"generations.json\",\n",
            "    \"save_references\": false,\n",
            "    \"save_references_path\": \"references.json\",\n",
            "    \"prompt\": \"prompt\",\n",
            "    \"max_memory_per_gpu\": null,\n",
            "    \"check_references\": false\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-2 base model results: MBPP\n",
        "!accelerate launch  main.py \\\n",
        "  --model \"microsoft/phi-2\" \\\n",
        "  --max_length_generation 256000 \\\n",
        "  --tasks mbpp \\\n",
        "  --temperature 0.2 \\\n",
        "  --n_samples 1 \\\n",
        "  --batch_size 1 \\\n",
        "  --allow_code_execution"
      ],
      "metadata": {
        "id": "7fbYDlS4OmRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}